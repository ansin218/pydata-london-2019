{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import operator\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import accumulate\n",
    "from functools import reduce\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipyparallel as ipp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras import backend as K\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from ipy_logger import create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "N_SENT_SAMPLES = 1000\n",
    "WANTED_COLS = ['Date', 'Hour', 'User Name', 'Tweet content', 'Is a RT', 'RTs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets(company):\n",
    "    df = pd.read_csv(f'tweets/{company}.csv', nrows=30000)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ipp.require(read_tweets, 're', 'string')\n",
    "def preprocess(company):\n",
    "    \n",
    "    df = read_tweets(company)\n",
    "    df = df[WANTED_COLS]\n",
    "    df = df[df['Is a RT'] == False]\n",
    "    \n",
    "    mention_regex = re.compile(r'@[A-Za-z0-9]+')\n",
    "    link_regex = re.compile(r'https?://[A-Za-z0-9./]+')\n",
    "    symbol_regex = re.compile(r'\\$[A-Za-z]+')\n",
    "\n",
    "    def remove_patterns(tweet, rgxs):\n",
    "        cleaned = tweet\n",
    "\n",
    "        for rgx in rgxs:\n",
    "            cleaned = re.sub(rgx, '', cleaned)\n",
    "        return cleaned\n",
    "\n",
    "    def clean_tweet(tweet):\n",
    "        clean = remove_patterns(tweet, [mention_regex, link_regex, symbol_regex])\n",
    "        clean = clean.replace('#', '').lower()\n",
    "        return clean\n",
    "\n",
    "    df['Clean Tweet'] = df['Tweet content'].apply(clean_tweet)\n",
    "    df['Clean Tweet'] = df['Clean Tweet'].str.replace('\\s+', ' ').str.replace('rt\\s+', '')\n",
    "    df['Clean Tweet'] = df['Clean Tweet'].str.translate(str.maketrans('', '', string.punctuation))    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Tasks\n",
    "\n",
    "1. Aggregate stats - number of tweets, average length, series of number of tweets/hour\n",
    "2. Histogram of lemmas\n",
    "3. Classification of sentiment of each tweet with model fitted on sentiment140 and sum up neg & pos tweets (see FitSentModel.ipynb for how the code of the model in Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(df):\n",
    "    avg_twt_len = round(df['Tweet content'].str.len().mean(), 0)\n",
    "\n",
    "    stats = {\n",
    "        'Count' : len(df),\n",
    "        'Total Character Count': df['Tweet content'].str.len().sum(),\n",
    "        'Time Period' : str(df.Date.min()) + ' -> ' + str(df.Date.max()),\n",
    "        'Average Tweet Length' : avg_twt_len,\n",
    "        'Average Number of RTs': round(df['RTs'].mean(), 0)\n",
    "    }\n",
    "\n",
    "    stats = pd.Series(stats, name='Tweet Stats')\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ipp.require(Counter)\n",
    "def get_top_lemmas(df, n):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    def count_lemmas(text):\n",
    "        lemmas = [lemmatizer.lemmatize(tok) for tok in tokenizer.tokenize(text)]\n",
    "        return Counter(lemmas)\n",
    "    \n",
    "    tweet_lemmas = Counter()\n",
    "\n",
    "    for tweet in df['Clean Tweet']:\n",
    "        tweet_lemmas += count_lemmas(tweet)\n",
    "        \n",
    "    top_n = dict(sorted(tweet_lemmas.items(), key=lambda x: -x[1])[:n])\n",
    "    return top_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De-serialize the sentiment classifier and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    keras_tokenizer = pickle.load(f)\n",
    "\n",
    "MODEL_MAXCHARS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_average_sentiment(df, sent_model=None):\n",
    "    \n",
    "    if sent_model is None:\n",
    "        from tensorflow.keras.models import load_model\n",
    "\n",
    "        ## Make sure to launch ipcluster in same directory\n",
    "        ## otherwise the root path will be different.\n",
    "        sent_model = load_model('sentmodel.h5')\n",
    "    \n",
    "    tweets = df['Clean Tweet'].sample(N_SENT_SAMPLES).values\n",
    "    processed_test_data = pad_sequences(keras_tokenizer.texts_to_sequences(tweets),\n",
    "                                        maxlen=MODEL_MAXCHARS)\n",
    "    classified_sentiment = sent_model.predict_classes(processed_test_data)\n",
    "    avg_sent = np.mean(classified_sentiment)\n",
    "    return avg_sent\n",
    "\n",
    "#print(f'Average sentiment of {N_SAMPLES} tweets about {company} : {avg_sent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Displaying Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_top_lemmas(top_lemmas, company):\n",
    "    %matplotlib inline\n",
    "    hist = plt.figure(figsize=(10,5))\n",
    "    plt.bar(top_lemmas.keys(), top_lemmas.values())\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.xlabel('Lemmas')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(company.capitalize())\n",
    "    plt.show()\n",
    "\n",
    "def show_results(company, stats, top_lemmas, avg_sent):\n",
    "    print('\\n' * 4)\n",
    "    print(f'Results for {company.upper()}')\n",
    "    print('Tweet Stats: ')\n",
    "    display(HTML(stats.to_frame().to_html()))\n",
    "    visualise_top_lemmas(top_lemmas, company)\n",
    "    print(f'Average sentiment of {N_SENT_SAMPLES} tweets: {avg_sent}')\n",
    "    \n",
    "def show_all(results):\n",
    "    for company, company_res in results.items():\n",
    "        show_results(company, *company_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running tasks sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['amzn', 'msft', 'nflx', 'googl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "results = {}\n",
    "sent_model = load_model('sentmodel.h5')\n",
    "\n",
    "for tick in tickers:\n",
    "    print(tick.upper())\n",
    "    df = preprocess(tick)\n",
    "    print('Running Task 1')\n",
    "    stats = get_stats(df)\n",
    "    print('Running Task 2')\n",
    "    top_lemmas = get_top_lemmas(df, 20)\n",
    "    print('Running Task 3')\n",
    "    avg_sent = find_average_sentiment(df, sent_model=sent_model)\n",
    "    results[tick] = (stats, top_lemmas, avg_sent)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f'Running Tasks sequentially took {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ipp.require('os', preprocess, get_stats, get_top_lemmas, find_average_sentiment, create_logger)\n",
    "def run_analysis(tick):\n",
    "    df = preprocess(tick)\n",
    "    stats = get_stats(df)\n",
    "\n",
    "    #logger = create_logger(tick)\n",
    "    #logger.info(f'Created stats for {tick}')\n",
    "    \n",
    "    top_lemmas = get_top_lemmas(df, 20)\n",
    "    avg_sent = find_average_sentiment(df)\n",
    "    return tick, (stats, top_lemmas, avg_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ipp.Client()\n",
    "dview = client[:]\n",
    "dview.execute('''\n",
    "              import pandas as pd\n",
    "              import numpy as np\n",
    "              \n",
    "              from nltk.stem.wordnet import WordNetLemmatizer\n",
    "              from nltk.tokenize import TweetTokenizer\n",
    "              from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "              ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview['keras_tokenizer'] = keras_tokenizer\n",
    "dview['N_SENT_SAMPLES'] = N_SENT_SAMPLES\n",
    "dview['WANTED_COLS'] = WANTED_COLS\n",
    "dview['MODEL_MAXCHARS'] = MODEL_MAXCHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "ipp_results = dview.map_sync(run_analysis, tickers)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f'Running Tasks in parallel took {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all(dict(ipp_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
